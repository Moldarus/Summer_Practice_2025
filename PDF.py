import pandas as pd
from pathlib import Path
import re
from razdel import sentenize
import csv
import numpy as np

# =============================================
# 1. –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò
# =============================================
INPUT_CSV = "output_data.csv"  # –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV
OUTPUT_CSV = "training_data.csv"
CHUNK_SIZE = 1000
MIN_CHUNK_LENGTH = 200
MAX_SENTENCES_PER_CHUNK = 5


# =============================================
# 2. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê
# =============================================
def clean_text(text):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if pd.isna(text) or not text.strip():
        return ""

    text = str(text)
    text = re.sub(r'[^\w\s.,:;!?()-]', ' ', text, flags=re.UNICODE)
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'\d+\s*$', '', text)
    text = text.replace('¬´', '"').replace('¬ª', '"').replace('‚Äî', '-').replace('‚Äì', '-')
    return text


def split_into_sentences(text):
    """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    return [s.text for s in sentenize(text)] if text else []


def create_chunks(sentences):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
    chunks = []
    current_chunk = []
    current_length = 0

    for sent in sentences:
        sent_length = len(sent)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —á–∞–Ω–∫–∞
        if (current_length + sent_length > CHUNK_SIZE or
                len(current_chunk) >= MAX_SENTENCES_PER_CHUNK):
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= MIN_CHUNK_LENGTH:
                chunks.append(chunk_text)
            current_chunk = []
            current_length = 0

        current_chunk.append(sent)
        current_length += sent_length

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        if len(chunk_text) >= MIN_CHUNK_LENGTH:
            chunks.append(chunk_text)

    return chunks


# =============================================
# 3. –û–ë–†–ê–ë–û–¢–ö–ê CSV –î–ê–¢–ê–°–ï–¢–ê
# =============================================
def process_csv():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ CSV –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    data = []
    print(f"üìñ –ß—Ç–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {INPUT_CSV}...")

    try:
        # –ß—Ç–µ–Ω–∏–µ CSV —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        df = pd.read_csv(
            INPUT_CSV,
            header=None,
            names=["PDF Name", "Extracted Text"],
            sep=",",  # –£—Ç–æ—á–Ω–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            on_bad_lines='skip'
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {str(e)}")
        return None

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    for index, row in df.iterrows():
        filename = row['PDF Name']
        text = row['Extracted Text']

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = clean_text(text)
        if not cleaned_text:
            continue

        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = split_into_sentences(cleaned_text)
        if not sentences:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑: {filename}")
            continue

        # –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        chunks = create_chunks(sentences)

        for i, chunk in enumerate(chunks):
            data.append({
                "source": filename,
                "text": chunk,
                "chunk_id": f"{Path(filename).stem}_{index}_{i}",
                "length": len(chunk),
                "num_sentences": chunk.count('.')  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
            })

        if index % 10 == 0:
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {index + 1}/{len(df)}")

    return data


# =============================================
# 4. –°–û–•–†–ê–ù–ï–ù–ò–ï –í CSV
# =============================================
def save_to_csv(data):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    if not data:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
        return False

    df = pd.DataFrame(data)

    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(df)}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {df['source'].nunique()}")
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {df['length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–ú–∏–Ω/–ú–∞–∫—Å –¥–ª–∏–Ω–∞: {df['length'].min()}/{df['length'].max()}")

    try:
        df.to_csv(
            OUTPUT_CSV,
            index=False,
            encoding="utf-8-sig",
            sep="|",
            quoting=csv.QUOTE_NONNUMERIC
        )
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_CSV}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}")
        return False


# =============================================
# 5. –ó–ê–ü–£–°–ö
# =============================================
if __name__ == "__main__":
    print("üöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    processed_data = process_csv()

    if processed_data:
        if save_to_csv(processed_data):
            print("\n–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            sample = pd.DataFrame(processed_data).head(3)
            print(sample[['source', 'chunk_id', 'length']])